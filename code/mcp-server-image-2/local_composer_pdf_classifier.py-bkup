
import os
import json
import argparse
import numpy as np
from pdf2image import convert_from_path
from PIL import Image
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import (
    GlobalAveragePooling2D, Dense, Dropout,
    RandomRotation, RandomZoom, RandomContrast
)
from tensorflow.keras.models import Model

def pdf_to_image(pdf_path: str, img_size: int):
    """Convert the first page of a PDF to a normalized RGB array."""
    try:
        pages = convert_from_path(pdf_path, first_page=1, last_page=1)
        if not pages:
            return None
        img = pages[0].convert("RGB").resize((img_size, img_size), Image.BICUBIC)
        return np.array(img, dtype=np.float32) / 255.0
    except Exception as e:
        print(f"⚠️ Skipping {pdf_path}: {e}")
        return None

def load_dataset(data_dir: str, img_size: int):
    """Recursively load all PDFs under data_dir/<composer>/... into (X, y, classes)."""
    X, y = [], []
    composers = sorted(
        d for d in os.listdir(data_dir)
        if os.path.isdir(os.path.join(data_dir, d))
    )
    if not composers:
        raise RuntimeError(f"No composer folders in {data_dir}")

    for comp in composers:
        comp_root = os.path.join(data_dir, comp)
        for root, _, files in os.walk(comp_root):
            for fn in files:
                if not fn.lower().endswith(".pdf"):
                    continue
                img = pdf_to_image(os.path.join(root, fn), img_size)
                if img is not None:
                    X.append(img)
                    y.append(comp)

    if not X:
        raise RuntimeError(f"No valid PDF images found under {data_dir}")

    X = np.stack(X)
    le = LabelEncoder()
    y_enc = le.fit_transform(y)
    return X, y_enc, list(le.classes_)

def build_transfer_model(input_shape, n_classes, lr=1e-4):
    """
    Build an EfficientNetB0-based classifier.
    Falls back to random init if pretrained weights unavailable.
    """
    try:
        base = EfficientNetB0(
            weights="imagenet",
            include_top=False,
            input_shape=input_shape
        )
        print("✅ Loaded EfficientNetB0 ImageNet weights")
    except Exception as e:
        print("⚠️ Could not load ImageNet weights, initializing from scratch:", e)
        base = EfficientNetB0(
            weights=None,
            include_top=False,
            input_shape=input_shape
        )

    base.trainable = False
    x = GlobalAveragePooling2D()(base.output)
    x = Dropout(0.5)(x)
    out = Dense(n_classes, activation="softmax")(x)

    model = Model(base.input, out)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(lr),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model, base

def train(args):
    # Load dataset
    X, y, classes = load_dataset(args.data_dir, args.img_size)
    print(f"Loaded {len(X)} samples across {len(classes)} composers: {classes}")

    # Decide on stratify vs random split
    n_samples, n_classes = len(y), len(classes)
    test_size = 0.2
    if n_samples * test_size >= 2 * n_classes:
        strat = y
    else:
        strat = None
        print("⚠️ Too few samples per class for stratified split; proceeding without stratification.")

    # Split
    X_tr, X_va, y_tr, y_va = train_test_split(
        X, y,
        test_size=test_size,
        stratify=strat,
        random_state=42
    )

    # Data augmentation
    augment = tf.keras.Sequential([
        RandomRotation(0.02),
        RandomZoom(0.1),
        RandomContrast(0.1),
    ])
    def prep(x, y): return augment(x, training=True), y

    tr_ds = (
        tf.data.Dataset.from_tensor_slices((X_tr, y_tr))
        .shuffle(len(X_tr))
        .map(prep, num_parallel_calls=tf.data.AUTOTUNE)
        .batch(args.batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )
    va_ds = (
        tf.data.Dataset.from_tensor_slices((X_va, y_va))
        .batch(args.batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )

    # Build & train with frozen backbone
    model, base = build_transfer_model(
        (args.img_size, args.img_size, 3),
        len(classes),
        lr=1e-4
    )
    print(f"▶ Training {args.epochs_freeze} epochs with frozen backbone")
    model.fit(tr_ds, validation_data=va_ds, epochs=args.epochs_freeze, verbose=2)

    # Fine-tune
    base.trainable = True
    for layer in base.layers[:-20]:
        layer.trainable = False
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-5),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    print(f"▶ Fine-tuning {args.epochs_finetune} epochs")
    model.fit(tr_ds, validation_data=va_ds, epochs=args.epochs_finetune, verbose=2)

    # Evaluate with full-report
    y_pred = np.argmax(model.predict(X_va), axis=1)
    all_labels = list(range(len(classes)))

    print("\n# Classification Report")
    print(classification_report(
        y_va,
        y_pred,
        labels=all_labels,
        target_names=classes,
        zero_division=0
    ))
    print("\n# Confusion Matrix")
    print(confusion_matrix(
        y_va,
        y_pred,
        labels=all_labels
    ))

    # Save model & labels
    model.save(args.model_out)
    with open(args.labels_out, "w") as f:
        json.dump(classes, f, indent=2)
    print(f"\n✅ Model saved to {args.model_out}")
    print(f"✅ Labels saved to {args.labels_out}")

def predict(args):
    model = tf.keras.models.load_model(args.model_path)
    classes = json.load(open(args.labels_path))
    img = pdf_to_image(args.pdf_path, args.img_size)
    if img is None:
        raise RuntimeError("Failed to convert PDF to image")
    prob = model.predict(img[np.newaxis,...], verbose=0)[0]
    idx = int(np.argmax(prob))
    print(f"Predicted composer: {classes[idx]} ({prob[idx]*100:.1f}% confidence)")

def main():
    parser = argparse.ArgumentParser(description="Composer classification from sheet-music PDFs")
    sub = parser.add_subparsers(dest="cmd", required=True)

    t = sub.add_parser("train")
    t.add_argument("--data_dir",      required=True, help="Root dir of PDFs (composer subdirs)")
    t.add_argument("--img_size",      type=int, default=224, help="Image size (px)")
    t.add_argument("--batch_size",    type=int, default=16)
    t.add_argument("--epochs_freeze", type=int, default=5)
    t.add_argument("--epochs_finetune", type=int, default=5)
    t.add_argument("--model_out",     default="composer_etl.h5")
    t.add_argument("--labels_out",    default="composer_labels.json")
    t.set_defaults(func=train)

    p = sub.add_parser("predict")
    p.add_argument("--pdf_path",    required=True, help="Path to a PDF to classify")
    p.add_argument("--model_path",  required=True, help="Path to saved .h5 model")
    p.add_argument("--labels_path", required=True, help="Path to labels .json")
    p.add_argument("--img_size",    type=int, default=224)
    p.set_defaults(func=predict)

    args = parser.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()

